<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Academic Projects ¬∑ Tony Zhang</title>
  <meta name="description" content="Tony Zhang ‚Äî Academic Projects" />
  <link href="styles.css" rel="stylesheet" />
</head>
<body>
  <!-- Topbar -->
  <div class="topbar">
    <div class="nav">
      <a class="brand" href="index.html">
        <div class="logo">
          <img src="circle.png" alt="Tony Zhang logo" />
        </div>
        <div>
          <div class="name">Tony Zhang</div>
          <div class="sub">Data Modeling¬∑ Analytics ¬∑ Visualization</div>
        </div>
      </a>
      <nav class="menu">
        <a href="experience.html">Working Experience</a>
        <a class="active" href="projects.html">Academic Project</a>
        <a href="teaching.html">Teaching Experience</a>
        <a href="contact.html">Contacts</a>
      </nav>
    </div>
  </div>

  <main class="wrap">
    <h1 class="page-title">Academic Projects</h1>
    <div class="grid">

      <!-- Songs Recommendation System -->
      <article class="card two shadow hover">
        <h3 style="margin-top:0">üéµ Songs Recommendation System</h3>
        <p class="muted">
          To enhance personalized music recommendations, I developed a system that streamed <strong>Spotify tracker data</strong> into a 
          <strong>PostgreSQL</strong> database.
        </p>
        <p class="muted">
          I connected the database using <strong>Python</strong> and employed <strong>SQL queries</strong> to clean and preprocess the data. 
          I then built and compared two machine learning models‚Äî<strong>Random Forest Regression</strong> and 
          <strong>Alternating Least Squares (ALS)</strong>‚Äîevaluating both <strong>implicit</strong and <strong>explicit rating</strong> strategies.
        </p>
        <p class="muted">
          After rigorous testing, the final model based on <strong>explicit ratings</strong> achieved the lowest 
          <strong>Root Mean Square Error (RMSE)</strong> of <strong>0.3364</strong>, demonstrating superior predictive accuracy and robustness.
        </p>
        <p><a class="btn" href="https://github.com/Chenzhang0329/Spotify-Music-Recommendation-Spark" target="_blank">GitHub Repo</a></p>
      </article>
      

      <!-- Stock Forecasting -->
      <article class="card two shadow hover">
        <h3 style="margin-top:0">üìà Stock Forecasting with a Hybrid LSTM Model</h3>
        <p class="muted">
          To explore the impact of <strong>news sentiment</strong> on stock price prediction, I developed a hybrid forecasting system combining 
          <strong>Long Short-Term Memory (LSTM)</strong> and <strong>Linear Regression</strong> models.
        </p>
        <p class="muted">
          Using <strong>R</strong>, I scraped financial news headlines for five major tech companies‚ÄîGoogle, Apple, Tesla, Amazon, and Microsoft‚Äîand applied 
          <strong>sentiment analysis</strong> to generate sentiment scores. These scores were normalized via <strong>min-max standardization</strong> and integrated with 
          historical stock price data collected through the <strong>Yahoo Finance API</strong> using <strong>Python</strong>.
        </p>
        <p class="muted">
          I conducted a comparative analysis of three models: standalone <strong>LSTM</strong>, <strong>Linear Regression</strong>, and a <strong>Hybrid LSTM + Linear Regression</strong>. 
          The standalone LSTM model achieved the best performance with a <strong>Root Mean Square Error (RMSE)</strong> of <strong>3.04</strong>, demonstrating its effectiveness 
          in capturing temporal dependencies in financial time series data.
        </p>
        <p><a class="btn" href="https://github.com/Chenzhang0329/StockPrice-Hybrid-Model-" target="_blank">GitHub Repo</a></p>
      </article>


      <!-- Automakers Stock Dashboard -->
      <article class="card two shadow hover">
        <h3 style="margin-top:0">üöó Dashboard for TOP48 Automakers‚Äô Stock Price</h3>
        <p class="muted">
          To support investor decision-making, I designed and developed an interactive dashboard using <strong>RShiny</strong>, 
          offering four key functionalities: <strong>statistical summary</strong>, <strong>historical stock price analysis</strong>, 
          <strong>comparison analysis</strong>, and <strong>stock forecasting</strong>.
        </p>
        <p class="muted">
          I implemented a dynamic <strong>candlestick chart</strong> with a movable timeline to visualize bullish and bearish trends. 
          For forecasting, I applied the <strong>ARIMA model</strong> and used <strong>auto.arima</strong> to automatically select the optimal model 
          for each automaker based on historical data.
        </p>
        <p class="muted">
          The completed dashboard was published on <strong>GitHub</strong> with a comprehensive <strong>README</strong> file, 
          providing users with clear instructions on how to navigate and utilize the tool effectively.
        </p>
        <p><a class="btn" href="https://github.com/Chenzhang0329/Dashboard-for-48-Automakers-stock-price" target="_blank">GitHub Repo</a></p>
      </article>

      <!-- Spaceship Titanic -->
      <article class="card two shadow hover">
        <h3 style="margin-top:0">üõ∏ Spaceship Titanic Classification on Kaggle</h3>
        <p class="muted">
          To improve classification accuracy for the <strong>Spaceship Titanic</strong> dataset on <strong>Kaggle</strong>, 
          I engineered new features such as a binary variable indicating whether a passenger was under 18 years old, which enhanced model performance.
        </p>
        <p class="muted">
          I identified right-skewed variables and transformed them into <strong>Bernoulli features</strong> to reduce noise‚Äîfor example, converting <strong>'Food Service'</strong> into <strong>'Zero Food Service'</strong> to indicate spending behavior. 
          I applied <strong>one-hot encoding</strong> to all categorical variables and trained a <strong>Logistic Regression</strong> model, achieving a mean cross-validation accuracy of <strong>0.79</strong>.
        </p>
        <p class="muted">
          I also built a <strong>Naive Bayes</strong> model by selecting features based on marginal accuracy, ultimately using all features to reach an accuracy of <strong>0.7732</strong>. 
          After comparing both models using <strong>precision</strong>, <strong>recall</strong>, and <strong>accuracy</strong>, 
          I submitted predictions using the logistic model and achieved a final Kaggle score of <strong>0.7896</strong>, placing in the <strong>top 50 percentile</strong> on the leaderboard.
        </p>
        <p><a class="btn" href="https://github.com/Chenzhang0329/Spaceship-Titanic-Classification/blob/main/data6100_proj2.ipynb" target="_blank">GitHub Repo</a></p>
      </article>

      <!-- House Price Prediction -->
      <article class="card shadow hover">
        <h3 style="margin-top:0">üè† House Price Prediction</h3>
        <p class="muted">
          To improve predictive accuracy for house prices, I cleaned the dataset by removing <strong>15 outliers</strong> and 
          filling missing values using the <strong>median</strong> for numerical variables and <strong>None Type</strong> for categorical variables. 
          This preprocessing was conducted using <strong>Python</strong> in <strong>Google Colab</strong>.
        </p>
        <p class="muted">
          I performed exploratory data analysis using <strong>Seaborn</strong> and <strong>Pandas</strong>, creating side-by-side 
          <strong>boxplots</strong>, <strong>scatterplots</strong>, and <strong>trajectory plots</strong> for both numerical and categorical variables. 
          A <strong>non-linear transformation</strong> was applied to the response variable to improve model fit.
        </p>
        <p class="muted">
          Categorical variables were converted into <strong>dummy variables</strong>, and I used <strong>cross-validation</strong> combined with 
          <strong>backward selection</strong> to identify the most efficient features based on <strong>Root Mean Squared Logarithmic Error (RMSLE)</strong>. 
          The final regression model predicted prices for <strong>1,459 houses</strong> on Kaggle‚Äôs test set, achieving a score of 
          <strong>0.1641</strong> and placing in the <strong>top 40 percentile</strong> on the leaderboard.
        </p>
        <p><a class="btn" href="https://github.com/Chenzhang0329/Spaceship-Titanic-Classification/blob/main/data6100_proj2.ipynb" target="_blank">GitHub Repo</a></p>
      </article>

    </div>
  </main>

  <footer>¬© <span id="year"></span> Tony Zhang</footer>
  <script>document.getElementById('year').textContent=new Date().getFullYear()</script>
</body>
</html>
